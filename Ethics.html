<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Generic - Forty by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><strong>Mavis Mnangagwa</strong> <span>ePortfolio</span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="landing.html">Landing</a></li>
							<li><a href="generic.html">Generic</a></li>
							<li><a href="elements.html">Elements</a></li>
						</ul>
						<ul class="actions stacked">
							<li><a href="#" class="button primary fit">Get Started</a></li>
							<li><a href="#" class="button fit">Log In</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h1>Reflective Activity 1: Ethics in Computing</h1>
									</header>
								
									<p><strong>Navigating the Generative AI Surge: A Reflection on Global Governance Principles and Professional Responsibilities</strong> 
									<li>
Late 2022 marked a watershed: public releases of ChatGPT and Stable Diffusion showcased models that could produce coherent text, images and even code on demand. While artificial intelligence (AI) research has existed for decades, these systems scaled capabilities and public adoption at an unprecedented speed. For computer‑science professionals the renaissance is both exhilarating and daunting: the same technologies that write boiler‑plate functions can also write malicious code or hallucinate legal citations. In such a fluid environment clear, enforceable and globally interoperable governance is no longer a luxury—it is a prerequisite for responsible innovation.
									</li>
									<li>
Corrêa et al.’s meta‑analysis of 200 ethics and policy documents finds a surprisingly robust “minimum bundle” of five high‑frequency principles—privacy, transparency, fairness, accountability and safety—but also exposes three gaps: (1) geographical skew towards the Global North, (2) heavy reliance on soft law (98 % voluntary codes) and (3) vague, sometimes contradictory terminology. In effect, we have normative convergence without practical convergence: everyone cites “transparency”, yet means anything from releasing source code to offering UX explanations. The authors call for tools that map, compare and ultimately operationalise these principles across jurisdictions.
									</li>
									<li> 
Richard Deckard’s BCS article shifts the lens from macro‑policy to the practitioner’s desktop. He frames AI ethics as a multidisciplinary craft that blends technical literacy with philosophy, communication and stakeholder engagement. His practical advice—continuous learning, contextual thinking, and clear communication—aligns neatly with Correa et al.’s diagnosis: it is precisely the interpretive gap between abstract principles and concrete implementation that skilled professionals must close. Deckard’s reminder that ethical competence is an ongoing practice, not a compliance checklist underpins the recommendations that follow.
									</li>
									<li>
Across jurisdictions, five leading AI producing regions are taking markedly different yet increasingly convergent approaches to generative AI governance. The European Union has enacted the Artificial Intelligence Act (2024), a hard law, risk based regime that obliges “systemic” foundation model providers to publish model cards, perform pre deployment evaluations, and register systems in an EU database. In the United States, a series of 2024 25 Executive Orders complements federal agency inventories of high risk AI with the voluntary yet influential NIST AI Risk Management Framework and the newly created AI Safety Institute, which is drafting a generative AI profile. The United Kingdom pursues a principles driven, regulator led approach outlined in its 2023 white paper, recently enriched by procurement specific guard rails for foundation models. China’s Interim Measures for the Management of Generative AI Services (August 2023) mandate provider registration, content filtering aligned with “socialist core values”, and disclosure of model and filing numbers. Finally, multilateral bodies such as the OECD (2019) and UNESCO (2021) have issued non binding recommendations increasingly cited in trade agreements and public procurement clauses. All of these instruments embed the five consensus principles privacy, transparency, fairness, accountability, and safety but vary in legal force and enforcement mechanics, producing a patchwork that organisations must navigate.
									</li>
									<li>
Drawing on these sources, mindful of legal, social and professional stakes, I advocate a layered, iterative governance model for organisations that build or deploy generative AI.
Adopt a “Minimum Ethical Baseline”: Institutionalise the five consensus principles as non negotiable gates in the software development life cycle (SDLC). Translate each into testable criteria e.g. fairness ↔ disparate impact analysis on synthetic data; transparency ↔ publish model and data sheet summaries.
Risk based Escalation: Classify use cases following the EU Act’s four tier logic (minimal, limited, high, unacceptable). High risk applications trigger independent audits and red team testing before release. This dovetails with the NIST AI RMF “Map Measure Manage” loop.
Multidisciplinary Ethics Board: Inspired by Deckard, create an internal board with engineers, legal, UX, and external civil society members. Its mandate: approve high risk deployments and oversee incident reporting.
Documentation & Traceability: Require model cards, data cards and lineage metadata for every foundation or fine tuned model. Align with ISO/IEC 42001 (AI management systems) for certification pathways.
Interoperability Mapping: Maintain a living matrix that cross walks EU, US, UK and Chinese requirements. This enables global product launches without regulatory lock in and supports privacy enhancing techniques where data transfer constraints exist.
Human Centred Continuous Learning: Allocate annual training budgets so practitioners can keep pace with rapid shifts in law and technology fulfilling BCS and ACM professional development obligations.

									</li>
									<li>
Legal. Embedding these processes lowers exposure to GDPR fines, product‑liability suits (EU) and copyright claims around training data. Early alignment with AI‑Act audits can become a market differentiator when the Act enters force in 2026.
Social. Explicit fairness metrics and redress mechanisms mitigate bias amplification, protect marginalised users and curb misinformation. Transparency artefacts foster explainability and thus public trust, addressing societal concerns identified by Corrêa et al.
Professional. Developers operate under clearer role expectations, reducing ethical distress and aligning with BCS/IEEE codes. Cross‑functional boards promote inclusive decision‑making, reflecting the gender‑diversity gap highlighted in the meta‑analysis.

									</li>
									<li>
Generative AI collapses the distance between research breakthrough and mass uptake. Corrêa et al. demonstrate that the world already agrees on what values matter; Deckard reminds professionals of how to realise them. Bridging the two demands actionable, risk‑based mechanisms, transparent documentation and a culture of continuous ethical reflexivity. By enshrining a layered governance model today, organisations can harness generative AI’s creative power while safeguarding legal compliance, social wellbeing and professional integrity.
									</li></p>
									
<p>References
<ul>Corrêa, N.K., Bietti, E., Elish, M.C., Narayan, A. and Sastry, G. (2023) ‘Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance’, Patterns, 4(11), Article 100639. https://doi.org/10.1016/j.patter.2023.100639</ul>
<ul>Cyberspace Administration of China (2023) Interim measures for the management of generative AI services. Available at: https://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm (Accessed 20 July 2025).</ul>
<ul>Deckard, R. (2023) ‘What are ethics in AI?’, BCS: Articles & Opinion, 3 April. Available at: https://www.bcs.org/articles-opinion-and-research/what-are-ethics-in-ai/ (Accessed 20 July 2025).</ul>
<ul>European Parliament and Council of the European Union (2024) Regulation (EU) 2024/1689 laying down harmonised rules on Artificial Intelligence (AI Act). Official Journal of the European Union, L …, pp. … Available at: https://eur-lex.europa.eu/ (Accessed 20 July 2025).</ul>
<ul>HM Government (2023) A pro-innovation approach to AI regulation: White paper. London: Department for Science, Innovation and Technology. Available at: https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach (Accessed 20 July 2025).</ul>
<ul>National Institute of Standards and Technology (NIST) (2023) Artificial intelligence risk management framework (AI RMF 1.0). Gaithersburg, MD: NIST. Available at: https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf (Accessed 20 July 2025).</ul>
<ul>Sanderson, C., Douglas, D. and Lu, Q. (2023) ‘Implementing responsible AI: Tensions and trade-offs between ethics aspects’, arXiv. Available at: https://arxiv.org/abs/2304.08275 (Accessed 20 July 2025).</ul>
<ul>UNESCO (2021) Recommendation on the ethics of artificial intelligence. Paris: UNESCO. Available at: https://unesdoc.unesco.org/ark:/48223/pf0000381137 (Accessed 20 July 2025).</ul>
<ul>U.S. Executive Office of the President (2025) Executive order on removing barriers to American leadership in AI. Washington, DC: The White House. Available at: https://www.whitehouse.gov/ (Accessed 20 July 2025).</ul>
<ul>World Economic Forum (2024) Governance in the age of generative AI: A 360° approach. Geneva: World Economic Forum. Available at: https://www.weforum.org/reports/governance-in-the-age-of-generative-ai (Accessed 20 July 2025).</ul>

										

									<p><strong>Reflection:</strong>Throughout this collaborative discussion, I gained valuable insights into how rapidly advancing technologies—particularly in healthcare—necessitate proactive rather than reactive cybersecurity measures. The WannaCry attack on the NHS in 2017 illustrated the substantial risks of operating legacy systems; one peer highlighted how the NHS’s reliance on unsupported Windows XP left it vulnerable (Acronis, 2020). This incident was preventable, underlining the importance of frequent updates and robust patch management. Another peer stressed that insufficient funding and delayed responses contributed to the NHS’s predicament, demonstrating how organisational culture and priorities can compound technical vulnerabilities.

I also realised that the integration of Industry 4.0 technologies, such as IoT and AI, introduces both efficiency gains and new risks (Schwab, 2016). While connected devices can enhance patient care, they expand the potential attack surface. Effective governance, clear policies, and continuous risk assessments are therefore crucial. Additionally, discussions about AI-powered writing tools (Hutson, 2021) revealed parallel themes: although AI can improve productivity and creativity, overreliance on automated systems may compromise authenticity and integrity in academic or professional settings.

Overall, I now appreciate that the balance between innovation and security hinges on strong leadership, adequate funding, and a culture of vigilance. By embracing forward-looking strategies and ethical guidelines, organisations can harness the benefits of emerging technologies without sacrificing security or public trust.





</p>
								</div>
							</section>

					</div>

			

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<ul class="icons">
								<li><a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
								<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
								<li><a href="#" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="#" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
							</ul>
							<ul class="copyright">
								<li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
